"""
Test Fall Detection v·ªõi Video MP4
S·ª≠ d·ª•ng: python test_fall_detection.py <ƒë∆∞·ªùng_d·∫´n_video.mp4> [--debug] [--save-output]

Options:
  --debug         Hi·ªÉn th·ªã th√¥ng tin debug chi ti·∫øt
  --save-output   L∆∞u video k·∫øt qu·∫£ v·ªõi annotation
"""

import cv2
import sys
import time
from pathlib import Path
import argparse
from datetime import datetime
import os
import yaml
import logging

# CRITICAL: Setup logging FIRST
logging.basicConfig(
    level=logging.DEBUG,  # Set to DEBUG to see all motion detection logs
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from yolo_fall_detector import YOLOFallDetector


def load_config():
    """Load configuration from YAML"""
    config_path = Path(__file__).parent / "config" / "config.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def test_fall_detection(video_path, debug=False, save_output=False):
    """Test fall detection v·ªõi video file"""
    
    # Ki·ªÉm tra file t·ªìn t·∫°i
    if not Path(video_path).exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y video: {video_path}")
        return
    
    # T·∫°o th∆∞ m·ª•c l∆∞u ·∫£nh t√© ng√£
    fall_images_dir = Path("fall_images")
    fall_images_dir.mkdir(exist_ok=True)
    
    print(f"üìπ ƒêang m·ªü video: {video_path}")
    print(f"üìÅ ·∫¢nh t√© ng√£ s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {fall_images_dir.absolute()}")
    
    # M·ªü video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("‚ùå Kh√¥ng th·ªÉ m·ªü video!")
        return
    
    # L·∫•y th√¥ng tin video
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"‚úÖ Video: {width}x{height} @ {fps}fps, T·ªïng {total_frames} frames")
    if debug:
        print(f"üêõ DEBUG MODE: Enabled")
    
    # Load config t·ª´ YAML
    print("üìã ƒêang load config t·ª´ config.yaml...")
    config = load_config()
    fall_config = config.get('fall_detection', {})
    
    # Kh·ªüi t·∫°o fall detector
    print("üîß ƒêang kh·ªüi t·∫°o Fall Detector...")
    fall_detector = YOLOFallDetector(fall_config)
    print("‚úÖ Fall Detector ƒë√£ s·∫µn s√†ng!")
    
    if debug:
        print("\nüìä TH√îNG S·ªê C·∫§U H√åNH (t·ª´ config.yaml):")
        print(f"  ‚Ä¢ Confidence threshold: {fall_config.get('conf_threshold', 0.5)}")
        print(f"  ‚Ä¢ Vertical speed threshold: {fall_config.get('fall_threshold', {}).get('vertical_speed', 0.3)}")
        print(f"  ‚Ä¢ Angle threshold: {fall_config.get('fall_threshold', {}).get('angle_threshold', 50)}¬∞")
        print(f"  ‚Ä¢ Duration threshold: {fall_config.get('fall_threshold', {}).get('duration_threshold', 0.3)}s")
        print(f"  ‚Ä¢ Cooldown: {fall_config.get('cooldown_seconds', 10)}s")
        print(f"  ‚Ä¢ Max missing frames: {fall_config.get('max_missing_frames', 10)} frames")
    
    # Video writer n·∫øu c·∫ßn save
    out = None
    if save_output:
        output_path = f"output_{Path(video_path).stem}.mp4"
        # Th·ª≠ H.264 codec (t·ªët h∆°n mp4v), fallback v·ªÅ XVID n·∫øu kh√¥ng c√≥
        fourcc = cv2.VideoWriter_fourcc(*'H264')  # ho·∫∑c 'avc1', 'x264'
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # Ki·ªÉm tra n·∫øu kh√¥ng m·ªü ƒë∆∞·ª£c, th·ª≠ XVID
        if not out.isOpened():
            print("‚ö†Ô∏è  H264 kh√¥ng kh·∫£ d·ª•ng, ƒëang th·ª≠ XVID...")
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            output_path = f"output_{Path(video_path).stem}.avi"
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        print(f"üíæ S·∫Ω l∆∞u output t·∫°i: {output_path}")
    
    # Stats
    frame_count = 0
    fall_count = 0
    fall_frames = []  # L∆∞u c√°c frame ph√°t hi·ªán t√© ng√£
    start_time = time.time()
    
    print("\n‚ñ∂Ô∏è  B·∫Øt ƒë·∫ßu x·ª≠ l√Ω video...")
    print("Nh·∫•n 'q' ƒë·ªÉ tho√°t, 'SPACE' ƒë·ªÉ t·∫°m d·ª´ng, 'd' ƒë·ªÉ toggle debug info")
    print("-" * 60)
    
    paused = False
    show_debug = debug
    
    while True:
        if not paused:
            ret, frame = cap.read()
            if not ret:
                print("\n‚úÖ ƒê√£ x·ª≠ l√Ω xong video!")
                break
            
            frame_count += 1
            
            # Process frame v·ªõi fall detector
            result = fall_detector.process_frame(frame)
            annotated_frame = result.get('annotated_frame', frame)
            state = result.get('state', 'unknown')
            
            # Hi·ªÉn th·ªã state
            if hasattr(state, 'value'):
                state = state.value
            
            # L·∫•y th√™m th√¥ng tin t·ª´ result
            poses = result.get('poses', [])
            motion_magnitude = result.get('motion_magnitude', 0.0)  # NEW: motion metric
            
            # Ki·ªÉm tra fall detected
            if result.get('fall_detected'):
                fall_count += 1
                fall_frames.append(frame_count)
                
                # L∆∞u ·∫£nh t√© ng√£
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                video_name = Path(video_path).stem
                image_filename = f"{video_name}_fall{fall_count}_{timestamp}_frame{frame_count}.jpg"
                image_path = fall_images_dir / image_filename
                
                cv2.imwrite(str(image_path), annotated_frame)
                
                # Ki·ªÉm tra n·∫øu l√† motion-based detection
                detection_type = "POSE" if poses else "MOTION"
                
                print(f"üö® T√â NG√É #{fall_count} [{detection_type}] t·∫°i frame {frame_count} ({frame_count/fps:.2f}s)")
                print(f"   üíæ ƒê√£ l∆∞u ·∫£nh: {image_filename}")
                
                if debug:
                    if poses:
                        pose = poses[0]
                        angle = pose.get('angle', 0)
                        vertical_speed = pose.get('vertical_speed', 0)
                        print(f"   üìê G√≥c: {angle:.1f}¬∞, T·ªëc ƒë·ªô: {vertical_speed:.3f}")
                    if motion_magnitude > 0:
                        print(f"   üåä Motion: {motion_magnitude:.3f}")
            
            # Th√™m th√¥ng tin l√™n frame
            y_offset = 30
            cv2.putText(annotated_frame, f"Frame: {frame_count}/{total_frames} ({frame_count/fps:.1f}s)", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            y_offset += 30
            
            # State v·ªõi m√†u t∆∞∆°ng ·ª©ng
            state_color = (0, 255, 255)  # Yellow
            if state == 'laying':
                state_color = (0, 0, 255)  # Red
            elif state == 'standing':
                state_color = (0, 255, 0)  # Green
                
            cv2.putText(annotated_frame, f"State: {state}", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, state_color, 2)
            y_offset += 30
            
            cv2.putText(annotated_frame, f"Falls Detected: {fall_count}", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            y_offset += 30
            
            # Hi·ªÉn th·ªã missing frames n·∫øu c√≥
            missing_frames = result.get('missing_frames', 0)
            if missing_frames > 0:
                cv2.putText(annotated_frame, f"Missing: {missing_frames} frames", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
                y_offset += 30
            
            # Debug info
            if show_debug and poses:
                pose = poses[0]
                angle = pose.get('angle', 0)
                vertical_speed = pose.get('vertical_speed', 0)
                acceleration = pose.get('acceleration', 0)
                stability = pose.get('stability', 0)
                fall_conf = pose.get('fall_confidence', 0)
                center_y = pose.get('center', [0, 0])[1]
                center_y_norm = pose.get('center_y_normalized', 0)
                
                # Display metrics
                cv2.putText(annotated_frame, f"Angle: {angle:.1f} deg", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                y_offset +=  25
                cv2.putText(annotated_frame, f"V.Speed: {vertical_speed:.3f}", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                y_offset += 25
                cv2.putText(annotated_frame, f"Accel: {acceleration:.3f}", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                y_offset += 25
                cv2.putText(annotated_frame, f"Stability: {stability:.2f}", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                y_offset += 25
                
                # Fall confidence with color coding
                conf_color = (0, 255, 0) if fall_conf < 0.4 else (0, 255, 255) if fall_conf < 0.6 else (0, 165, 255) if fall_conf < 0.8 else (0, 0, 255)
                cv2.putText(annotated_frame, f"Fall Risk: {fall_conf:.2f}", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, conf_color, 2)
                y_offset += 25
                
                cv2.putText(annotated_frame, f"Center Y: {center_y:.0f} ({center_y_norm:.2f})", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                y_offset += 25
                
                # Position indicator
                position_text = "HIGH" if center_y_norm < 0.5 else "MID" if center_y_norm < 0.7 else "LOW (ground)"
                position_color = (0, 255, 0) if center_y_norm < 0.5 else (0, 255, 255) if center_y_norm < 0.7 else (0, 165, 255)
                cv2.putText(annotated_frame, f"Position: {position_text}", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, position_color, 1)
                y_offset += 25
            
            # NEW: Always show motion magnitude (works without bounding box)
            if show_debug and motion_magnitude > 0:
                motion_color = (0, 255, 0) if motion_magnitude < 0.1 else (0, 255, 255) if motion_magnitude < 0.2 else (0, 0, 255)
                cv2.putText(annotated_frame, f"Motion: {motion_magnitude:.3f}", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, motion_color, 2)
                y_offset += 25
            
            # L∆∞u video n·∫øu c·∫ßn
            if out is not None:
                out.write(annotated_frame)
            
            # Hi·ªÉn th·ªã frame
            cv2.imshow('Fall Detection Test (Press \'h\' for help)', annotated_frame)
        
        # Keyboard controls
        key = cv2.waitKey(1 if not paused else 0) & 0xFF
        if key == ord('q'):
            print("\n‚èπÔ∏è  D·ª´ng b·ªüi ng∆∞·ªùi d√πng")
            break
        elif key == ord(' '):
            paused = not paused
            if paused:
                print(f"‚è∏Ô∏è  T·∫°m d·ª´ng t·∫°i frame {frame_count}")
            else:
                print("‚ñ∂Ô∏è  Ti·∫øp t·ª•c")
        elif key == ord('d'):
            show_debug = not show_debug
            print(f"üêõ Debug info: {'ON' if show_debug else 'OFF'}")
        elif key == ord('h'):
            print("\n‚å®Ô∏è  PH√çM T·∫ÆT:")
            print("  SPACE - T·∫°m d·ª´ng/Ti·∫øp t·ª•c")
            print("  d     - B·∫≠t/T·∫Øt debug info")
            print("  q     - Tho√°t")
            print("  h     - Hi·ªÉn th·ªã help\n")
    
    # Cleanup
    cap.release()
    if out is not None:
        out.release()
        print(f"üíæ ƒê√£ l∆∞u video output!")
    cv2.destroyAllWindows()
    
    # Summary
    elapsed = time.time() - start_time
    print("\n" + "=" * 60)
    print("üìä K·∫æT QU·∫¢ TEST:")
    print(f"  ‚Ä¢ T·ªïng frames x·ª≠ l√Ω: {frame_count}")
    print(f"  ‚Ä¢ S·ªë l·∫ßn ph√°t hi·ªán t√© ng√£: {fall_count}")
    if fall_frames:
        print(f"  ‚Ä¢ C√°c frame ph√°t hi·ªán: {fall_frames}")
        print(f"  ‚Ä¢ Th·ªùi ƒëi·ªÉm (gi√¢y): {[f'{f/fps:.2f}s' for f in fall_frames]}")
    print(f"  ‚Ä¢ Th·ªùi gian x·ª≠ l√Ω: {elapsed:.2f}s")
    print(f"  ‚Ä¢ FPS trung b√¨nh: {frame_count/elapsed:.2f}")
    print("=" * 60)
    
    # Hi·ªÉn th·ªã config ƒë√£ d√πng v√† g·ª£i √Ω
    config = load_config()
    fall_config_used = config.get('fall_detection', {})
    
    if fall_count == 0:
        print("\n‚ö†Ô∏è  KH√îNG PH√ÅT HI·ªÜN T√â NG√É N√ÄO!")
        print("üí° G·ª£i √Ω ƒëi·ªÅu ch·ªânh trong config.yaml:")
        current_speed = fall_config_used.get('fall_threshold', {}).get('vertical_speed', 0.3)
        current_angle = fall_config_used.get('fall_threshold', {}).get('angle_threshold', 50)
        print(f"  - Gi·∫£m vertical_speed (hi·ªán t·∫°i: {current_speed})")
        print(f"  - TƒÉng angle_threshold (hi·ªán t·∫°i: {current_angle}¬∞)")
        print("  - Ch·∫°y l·∫°i v·ªõi --debug ƒë·ªÉ xem chi ti·∫øt")
    elif fall_count > 1:
        print("\n‚ö†Ô∏è  PH√ÅT HI·ªÜN NHI·ªÄU L·∫¶N!")
        print("üí° G·ª£i √Ω ƒëi·ªÅu ch·ªânh trong config.yaml:")
        current_cooldown = fall_config_used.get('cooldown_seconds', 10)
        current_speed = fall_config_used.get('fall_threshold', {}).get('vertical_speed', 0.3)
        print(f"  - TƒÉng cooldown_seconds (hi·ªán t·∫°i: {current_cooldown}s)")
        print(f"  - TƒÉng vertical_speed (hi·ªán t·∫°i: {current_speed})")
  

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Test Fall Detection v·ªõi video MP4',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª•:
  python test_fall_detection.py video.mp4
  python test_fall_detection.py video.mp4 --debug
  python test_fall_detection.py video.mp4 --debug --save-output
  
Ph√≠m t·∫Øt khi ch·∫°y:
  SPACE - T·∫°m d·ª´ng/Ti·∫øp t·ª•c
  d     - B·∫≠t/T·∫Øt debug info
  q     - Tho√°t
  h     - Hi·ªÉn th·ªã help
        """
    )
    
    parser.add_argument('video', help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn file video MP4')
    parser.add_argument('--debug', action='store_true', 
                       help='Hi·ªÉn th·ªã th√¥ng tin debug chi ti·∫øt (g√≥c nghi√™ng, t·ªëc ƒë·ªô, ...)')
    parser.add_argument('--save-output', action='store_true',
                       help='L∆∞u video k·∫øt qu·∫£ v·ªõi annotation')
    
    args = parser.parse_args()
    
    test_fall_detection(args.video, debug=args.debug, save_output=args.save_output)
